{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(root=\"data/\", download=True, transform = ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x251c51f6c10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img,label = dataset[0]\n",
    "print(img.shape)\n",
    "plt.imshow(img[0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split indices for 20% validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_indices(n, val_pct):\n",
    "    n_val = int(n*val_pct)\n",
    "    idxs = np.random.permutation(n)\n",
    "    return idxs[n_val:], idxs[:n_val] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 12000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indices, val_indices = split_indices(len(dataset), 0.2)\n",
    "len(train_indices),len(val_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_sample = SubsetRandomSampler(train_indices)\n",
    "train_dl = DataLoader(dataset, batch_size, sampler=train_sample)\n",
    "\n",
    "valid_sample = SubsetRandomSampler(val_indices)\n",
    "valid_dl = DataLoader(dataset, batch_size, sampler=valid_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We are now ready to define our model. As discussed above, we'll create a neural network with one hidden layer. Here's what that means:\n",
    "\n",
    "* Instead of using a single `nn.Linear` object to transform a batch of inputs (pixel intensities) into outputs (class probabilities), we'll use two `nn.Linear` objects. Each of these is called a _layer_ in the network. \n",
    "\n",
    "* The first layer (also known as the hidden layer) will transform the input matrix of shape `batch_size x 784` into an intermediate output matrix of shape `batch_size x hidden_size`. The parameter `hidden_size` can be configured manually (e.g., 32 or 64).\n",
    "\n",
    "* We'll then apply a non-linear *activation function* to the intermediate outputs. The activation function transforms individual elements of the matrix.\n",
    "\n",
    "* The result of the activation function, which is also of size `batch_size x hidden_size`, is passed into the second layer (also known as the output layer).  The second layer transforms it into a matrix of size `batch_size x 10`. We can use this output to compute the loss and adjust weights using gradient descent.\n",
    "\n",
    "\n",
    "As discussed above, our model will contain one hidden layer. Here's what it looks like visually:\n",
    "<center>\n",
    "<img src=\"https://i.imgur.com/eN7FrpF.png\" width=\"480\">\n",
    "</center>\n",
    "\n",
    "Let's define the model by extending the `nn.Module` class from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a custom class to set our layers as well as avoid the problem that we faced regarding the shape of our image tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self,in_size, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        # hidden layer\n",
    "        self.linear1 = nn.Linear(in_size, hidden_size)\n",
    "        # output layer\n",
    "        self.linear2 = nn.Linear(hidden_size, out_size)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        # flatter tensor\n",
    "        xb = xb.view(xb.size(0),-1)\n",
    "        # apply hidden layer\n",
    "        out = self.linear1(xb)\n",
    "        # apply ReLU\n",
    "        out = F.relu(out)\n",
    "        # apply out layer\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will create a model with hidden size = 32\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "\n",
    "model = MnistModel(input_size, 32, out_size = num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting an output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3176956176757812\n",
      "Outputs Size: torch.Size([100, 10])\n",
      "Outputs egs: tensor([[-0.0617,  0.0867,  0.0536,  0.1817,  0.0318,  0.2732, -0.1201, -0.0592,\n",
      "         -0.1365, -0.1753],\n",
      "        [-0.1375,  0.1219, -0.0791,  0.2446,  0.0974,  0.2420, -0.0980,  0.0808,\n",
      "         -0.1921, -0.1883]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_dl:\n",
    "    outputs = model(images)\n",
    "    loss = F.cross_entropy(outputs, labels)\n",
    "    print(\"Loss:\",loss.item())\n",
    "    break;\n",
    "\n",
    "print(\"Outputs Size:\",outputs.shape)\n",
    "print(\"Outputs egs:\",outputs[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if GPU is available and we have cuda drivers installed we check by the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to assign a device that will use cuda or cpu depending on what is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving Data and Model to Device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the device is created, we will move our data and model to the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    if isinstance(data,(list,tuple)):\n",
    "        return [to_device(x,device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_dl:\n",
    "    print(images.shape)\n",
    "    images = to_device(images, device)\n",
    "    print(images.device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is usually a bad idea to put everything into the GPU at once. It is not ideal to put 10s of GB of data onto the GPU. Hence we use a class that loads them onto the GPU as and when required. \n",
    "There is no need to remove from GPU as it has an automatic garbage collection system, to remove items not in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        # creating objects for data loader and device but not putting them on the GPU\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "    \n",
    "    def __iter__(self):\n",
    "        # used to put data onto device from dataloader one at a time \n",
    "        for b in self.dl:\n",
    "            yield to_device(b,self.device)\n",
    "        # yield is used to return like a generator. \n",
    "        # Meaning that it will execute on for loop using memory and stop all other executions and then use the same memory \n",
    "        # for the other element of for loop\n",
    "    \n",
    "    def __len__(self):\n",
    "        print(len(self.dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "valid_dl = DeviceDataLoader(valid_dl, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if tensors have been moved onto our device we can do it by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "tensor([0, 2, 7, 2, 7, 6, 5, 7, 8, 3, 7, 7, 1, 4, 4, 3, 3, 7, 9, 4, 1, 5, 6, 0,\n",
      "        6, 6, 1, 9, 8, 3, 6, 2, 8, 0, 7, 3, 8, 2, 0, 9, 7, 1, 7, 3, 3, 3, 4, 3,\n",
      "        7, 6, 8, 0, 5, 3, 7, 0, 2, 3, 3, 9, 5, 0, 2, 6, 2, 2, 8, 9, 0, 1, 9, 8,\n",
      "        6, 2, 4, 0, 7, 9, 0, 2, 6, 6, 6, 6, 7, 6, 3, 6, 7, 9, 9, 2, 8, 8, 8, 8,\n",
      "        2, 9, 5, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for xb,yb in valid_dl:\n",
    "    print(xb.device)\n",
    "    print(yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the loss_batch, evaluate, and fit that we used in logistic regression again. They will be exactly same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_fn, xb,yb, opt=None, metric = None):\n",
    "    pred = model(xb)\n",
    "    loss = loss_fn(pred, yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    metric_result = None\n",
    "    if metric is not None:\n",
    "        metric_result = metric(pred,yb)\n",
    "        \n",
    "    return loss.item(), len(xb), metric_result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, valid_dl, metric=None):\n",
    "    with torch.no_grad():\n",
    "        results = [loss_batch(model,loss_fn,xb,yb,metric = metric) for xb,yb in valid_dl]\n",
    "        \n",
    "        losses, nums, metrics = zip(*results)\n",
    "        \n",
    "        total = np.sum(nums)\n",
    "        \n",
    "        avg_loss = np.sum(np.multiply(losses,nums))/total \n",
    "        avg_metric = None\n",
    "        \n",
    "        if metric is not None:\n",
    "            avg_metric = np.sum(np.multiply(metrics,nums))/total \n",
    "        \n",
    "        return avg_loss, total, avg_metric\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr, model, loss_fn, train_dl, valid_dl, metric=None, opt_fn = None):\n",
    "    losses, metrics = [], []\n",
    "    \n",
    "    # Making the optimizer \n",
    "    if opt_fn is None: \n",
    "        opt_fn = torch.optim.SGD\n",
    "    opt = opt_fn(model.parameters(), lr= lr)\n",
    "    \n",
    "    # looping over epochs\n",
    "    for epoch in range(epochs):\n",
    "        # training\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_fn, xb, yb, opt = opt)\n",
    "    \n",
    "        # Evaluation\n",
    "        result = evaluate(model, loss_fn, valid_dl, metric)\n",
    "        val_loss, total, val_metric = result\n",
    "        \n",
    "        losses.append(val_loss)\n",
    "        metrics.append(val_metric)\n",
    "        \n",
    "        if metric is None:\n",
    "            print ('Epoch [{}/{}], Loss: {:.4f}'.format (epoch+1, epochs, val_loss))\n",
    "        else:\n",
    "            print ('Epoch [{}/{}], Loss: {:.4f}, {}: {:.4f}'.format (epoch+1, epochs, val_loss, metric.__name__, val_metric))\n",
    "        \n",
    "    return losses, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _,preds  = torch.max(outputs,dim=1)\n",
    "    return torch.sum(preds==labels).item()/len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MnistModel(\n",
       "  (linear1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (linear2): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We put the model onto the gpu\n",
    "model = MnistModel(input_size, 32, out_size=num_classes)\n",
    "to_device(model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.310441529750824 Accuracy: 0.11291666666666667\n"
     ]
    }
   ],
   "source": [
    "val_loss, total, val_acc = evaluate(model, F.cross_entropy, valid_dl, accuracy)\n",
    "print(\"Loss:\",val_loss,\"Accuracy:\", val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial accuracy is 10% cz it is randomly initialized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.2311, accuracy: 0.9313\n",
      "Epoch [2/5], Loss: 0.1949, accuracy: 0.9442\n",
      "Epoch [3/5], Loss: 0.1629, accuracy: 0.9534\n",
      "Epoch [4/5], Loss: 0.1507, accuracy: 0.9568\n",
      "Epoch [5/5], Loss: 0.1413, accuracy: 0.9613\n"
     ]
    }
   ],
   "source": [
    "loss1, metric1 = fit(5, 0.5,model, F.cross_entropy, train_dl, valid_dl, metric=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It jumped to 93% accuracy in just one epoch and ended with 96% accuracy by the end of only 5 epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.1239, accuracy: 0.9657\n",
      "Epoch [2/5], Loss: 0.1219, accuracy: 0.9653\n",
      "Epoch [3/5], Loss: 0.1228, accuracy: 0.9650\n",
      "Epoch [4/5], Loss: 0.1205, accuracy: 0.9672\n",
      "Epoch [5/5], Loss: 0.1210, accuracy: 0.9660\n"
     ]
    }
   ],
   "source": [
    "loss2, metric2 = fit(5, 0.1,model, F.cross_entropy, train_dl, valid_dl, metric=accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it jumped to about 96.6%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'accuracy')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can have an accuracy graph as \n",
    "accuracies = [val_acc] + metric1 + metric2\n",
    "plt.plot(accuracies, \"-x\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that the plot flattens around 95-96%. To improve the accuracy we can either\n",
    "1. Increase size of hidden batch\n",
    "2. Add more hidden layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the data\n",
    "test_dataset = MNIST(root= \"data/\", train=False, transform = ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert to dataloader \n",
    "test_dl = DataLoader(test_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move dataset to device\n",
    "test_dl = DeviceDataLoader(test_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9678\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in test_dl:\n",
    "    test_loss,_,test_acc = evaluate(model, F.cross_entropy, test_dl, accuracy)\n",
    "    print(test_acc)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dcacb0086e9a4f4eabd41c33bf4faac5ea0a3337ed3f5eff0680afa930572c04"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
