{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.3%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\train-images-idx3-ubyte.gz to data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102.8%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST\\raw\\train-labels-idx1-ubyte.gz to data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2.9%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data/MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = MNIST(root=\"data/\",download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = MNIST(root=\"data/\",train=False)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28 at 0x2203A794F10>, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image,label = dataset[0]\n",
    "plt.imshow(image, cmap='gray')\n",
    "print(\"Label: \",label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image,label = dataset[1]\n",
    "plt.imshow(image, cmap='gray')\n",
    "print(\"Label: \",label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform img to Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to convert these images to tensors so that pytorch can work with them. For that we have the torch vision library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(root=\"data/\", train = True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n"
     ]
    }
   ],
   "source": [
    "img_tensor, label = dataset[0]\n",
    "print(img_tensor.shape, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dimension tells us the colors, the next two give us the height and width. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
      "         [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
      "         [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
      "         [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
      "         [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]]])\n"
     ]
    }
   ],
   "source": [
    "print(img_tensor[:,10:15,10:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get a 5X5 matrix which shows us the color of a particular excerpt of the img. And have values from 0 to 1\n",
    "\n",
    "Value of 1 represents white<br>\n",
    "Value of 0 represents black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2203fd748b0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to show image of selected area\n",
    "plt.imshow(img_tensor[0,10:20,10:20], cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation Datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While building real-world machine learning models, it is quite common to split the dataset into three parts:\n",
    "\n",
    "1. **Training set** - used to train the model, i.e., compute the loss and adjust the model's weights using gradient descent.\n",
    "2. **Validation set** - used to evaluate the model during training, adjust hyperparameters (learning rate, etc.), and pick the best version of the model.\n",
    "3. **Test set** - used to compare different models or approaches and report the model's final accuracy.\n",
    "\n",
    "In the MNIST dataset, there are 60,000 training images and 10,000 test images. The test set is standardized so that different researchers can report their models' results against the same collection of images. \n",
    "\n",
    "Since there's no predefined validation set, we must manually split the 60,000 images into training and validation datasets. Let's set aside 10,000 randomly chosen images for validation. We can do this using the `random_spilt` method from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function to split test into test and validation sets using np\n",
    "import numpy as np\n",
    "def split_indices(n, val_percent): # n is total number of values in train set. val_percent is percent to be used for validation set\n",
    "    n_validation = int(n * val_percent) # number of entries in validation set\n",
    "    random_indices = np.random.permutation(n) # gives a set of values from 0 to n which are randonly selected\n",
    "    return random_indices[n_validation:], random_indices[:n_validation] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, val_indices = split_indices(len(dataset), 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we have training set and validation set indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "train_loader = DataLoader(dataset, batch_size, sampler = train_sampler)\n",
    "\n",
    "\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "val_loader = DataLoader(dataset, batch_size, sampler = val_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model will be quite similar to Linear Regression. Logistic Regression in itself is similar to Linear Regression. \n",
    "\n",
    "We will hence be using the the pred to be: <br>\n",
    "<strong> pred = x @ w.t() + b </strong>\n",
    "\n",
    "So we can do this via nn.Linear. <br>\n",
    "Along wit this we will flatten the [1,28,28] image to be a vector of size 784(28*28) and this will be our input. <br>\n",
    "The output will be a vector of size 10 with each element predicting the probability of number being of its respective index. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28\n",
    "num_classes = 10\n",
    "model = nn.Linear(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0148, -0.0026, -0.0340,  ...,  0.0343,  0.0289,  0.0115],\n",
       "        [ 0.0228,  0.0347,  0.0156,  ...,  0.0055,  0.0026,  0.0265],\n",
       "        [ 0.0029,  0.0024,  0.0059,  ..., -0.0095, -0.0331, -0.0212],\n",
       "        ...,\n",
       "        [-0.0173,  0.0267,  0.0022,  ...,  0.0095, -0.0345, -0.0031],\n",
       "        [-0.0022,  0.0154,  0.0211,  ..., -0.0007,  0.0038, -0.0031],\n",
       "        [-0.0028, -0.0129,  0.0059,  ..., -0.0289, -0.0022, -0.0285]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.weight.shape)\n",
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n",
      "Parameter containing:\n",
      "tensor([-4.3022e-03, -5.3514e-05, -2.6088e-02, -2.9646e-02, -2.8009e-02,\n",
      "         1.7267e-03,  1.3888e-02, -2.2585e-02,  2.5290e-02, -7.5793e-03],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.bias.shape)\n",
    "print(model.bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "Now we will run into an issue. Because our image is of shape [1,28,28] and hence cannot be multiplied by our weights which is a vector of 784 size. \n",
    "So we will need to .reshape() the image. \n",
    "\n",
    "So we will need to create a custom model, which can facilitate the same. So here we will make another class which will serve as our new model and it will be made by extending the nn.Module class. \n",
    "We will then assign create an instance of this class as our new model and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes) # we are creating a nn.Linear model here by default. \n",
    "        # which can be referenced via the linear parameter of the class.\n",
    "    \n",
    "    def forward(self,xb):\n",
    "        xb = xb.reshape(-1,784) \n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also means that we cannot access our weights and models from the model class directly. We need to access it via the linear in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=10, bias=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[-0.0342, -0.0287,  0.0265,  ...,  0.0150, -0.0176,  0.0183],\n",
       "         [ 0.0035, -0.0207, -0.0131,  ...,  0.0092, -0.0085,  0.0298],\n",
       "         [-0.0257, -0.0132,  0.0212,  ...,  0.0069, -0.0151,  0.0301],\n",
       "         ...,\n",
       "         [-0.0064, -0.0187,  0.0025,  ..., -0.0093, -0.0320, -0.0204],\n",
       "         [ 0.0271,  0.0242,  0.0047,  ...,  0.0140, -0.0136, -0.0032],\n",
       "         [-0.0352,  0.0272,  0.0117,  ...,  0.0199,  0.0342, -0.0324]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0180, -0.0167,  0.0276, -0.0227, -0.0009, -0.0196, -0.0128,  0.0111,\n",
       "          0.0304,  0.0212], requires_grad=True))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.weight, model.linear.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0342, -0.0287,  0.0265,  ...,  0.0150, -0.0176,  0.0183],\n",
       "         [ 0.0035, -0.0207, -0.0131,  ...,  0.0092, -0.0085,  0.0298],\n",
       "         [-0.0257, -0.0132,  0.0212,  ...,  0.0069, -0.0151,  0.0301],\n",
       "         ...,\n",
       "         [-0.0064, -0.0187,  0.0025,  ..., -0.0093, -0.0320, -0.0204],\n",
       "         [ 0.0271,  0.0242,  0.0047,  ...,  0.0140, -0.0136, -0.0032],\n",
       "         [-0.0352,  0.0272,  0.0117,  ...,  0.0199,  0.0342, -0.0324]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0180, -0.0167,  0.0276, -0.0227, -0.0009, -0.0196, -0.0128,  0.0111,\n",
       "          0.0304,  0.0212], requires_grad=True)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters()) # we can do this as we are extending the nn.Module class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will pass the images into our model to get the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_loader: \n",
    "    outputs = model(images)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1208, -0.2691, -0.5300, -0.1142, -0.0899,  0.3923,  0.1140,  0.1554,\n",
      "          0.0325, -0.3034],\n",
      "        [-0.1093, -0.0547, -0.2469, -0.0045,  0.1374,  0.3097,  0.2427,  0.2564,\n",
      "         -0.1285, -0.1923]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SoftMax Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that the output we get are in negative. This in no way shows probabilities. Not only that but, the sum of all elems of target isnt 1. \n",
    "\n",
    "To bring it into probability form we use the SoftMax probability which is easy to implement and use.  \n",
    "\n",
    "![softmax](https://i.imgur.com/EAh9jLN.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1148, 0.0777, 0.0599, 0.0908, 0.0930, 0.1506, 0.1140, 0.1189, 0.1051,\n",
      "        0.0751])\n",
      "0.9999999403953552\n"
     ]
    }
   ],
   "source": [
    "probs = F.softmax(outputs, dim = 1)\n",
    "print(probs[0].data)\n",
    "print(torch.sum(probs[0]).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max of the probabilities will be the output prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 5, 6, 5, 5, 3, 8, 5, 5, 5, 5, 6, 5, 5, 5, 5, 4, 6, 5, 6, 5, 5, 5, 3,\n",
      "        9, 7, 5, 5, 5, 8, 5, 5, 3, 5, 5, 5, 8, 5, 1, 5, 9, 1, 8, 5, 5, 3, 3, 1,\n",
      "        5, 5, 6, 5, 5, 5, 5, 8, 5, 7, 5, 5, 5, 8, 8, 1, 0, 6, 4, 5, 5, 3, 5, 3,\n",
      "        8, 5, 7, 5, 1, 1, 5, 7, 5, 5, 1, 5, 5, 6, 8, 6, 5, 5, 3, 5, 5, 5, 6, 5,\n",
      "        5, 5, 1, 6])\n"
     ]
    }
   ],
   "source": [
    "max_probs, preds = torch.max(probs, dim=1)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 8, 0, 7, 3, 2, 6, 1, 1, 4, 9, 7, 4, 3, 3, 3, 6, 6, 1, 8, 4, 5, 2, 4,\n",
       "        7, 3, 5, 6, 6, 9, 3, 0, 4, 8, 3, 7, 7, 1, 1, 0, 7, 6, 6, 9, 3, 4, 7, 2,\n",
       "        9, 9, 6, 8, 7, 1, 8, 3, 8, 5, 5, 5, 2, 2, 9, 8, 0, 7, 6, 8, 8, 6, 4, 2,\n",
       "        4, 5, 3, 9, 2, 2, 6, 1, 9, 6, 4, 6, 6, 6, 7, 7, 8, 8, 8, 3, 6, 5, 7, 4,\n",
       "        5, 2, 1, 6])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is normal to see that our predictions dont match our targets. This is because the initial weights and biases taken were random.\n",
    "Once we apply loss functions and change our model bit by bit we will see our predictions match targets better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics and Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to have an accuracy and loss measure to make changes to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(l1, l2):\n",
    "    return torch.sum(l1==l2).item()/len(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function - Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is an excellent way for us (humans) to evaluate the model. However, we can't use it as a loss function for optimizing our model using gradient descent for the following reasons:\n",
    "\n",
    "1. It's not a differentiable function. `torch.max` and `==` are both non-continuous and non-differentiable operations, so we can't use the accuracy for computing gradients w.r.t the weights and biases.\n",
    "\n",
    "2. It doesn't take into account the actual probabilities predicted by the model, so it can't provide sufficient feedback for incremental improvements. \n",
    "\n",
    "For these reasons, accuracy is often used as an **evaluation metric** for classification, but not as a loss function. A commonly used loss function for classification problems is the **cross-entropy**, which has the following formula:\n",
    "\n",
    "![cross-entropy](https://i.imgur.com/VDRDl1D.png)\n",
    "\n",
    "While it looks complicated, it's actually quite simple:\n",
    "\n",
    "* For each output row, pick the predicted probability for the correct label. E.g., if the predicted probabilities for an image are `[0.1, 0.3, 0.2, ...]` and the correct label is `1`, we pick the corresponding element `0.3` and ignore the rest.\n",
    "\n",
    "* Then, take the [logarithm](https://en.wikipedia.org/wiki/Logarithm) of the picked probability. If the probability is high, i.e., close to 1, then its logarithm is a very small negative value, close to 0. And if the probability is low (close to 0), then the logarithm is a very large negative value. We also multiply the result by -1, which results is a large postive value of the loss for poor predictions.\n",
    "\n",
    "![](https://www.intmath.com/blog/wp-content/images/2019/05/log10.png)\n",
    "\n",
    "* Finally, take the average of the cross entropy across all the output rows to get the overall loss for a batch of data.\n",
    "\n",
    "Unlike accuracy, cross-entropy is a continuous and differentiable function. It also provides useful feedback for incremental improvements in the model (a slightly higher probability for the correct label leads to a lower loss). These two factors make cross-entropy a better choice for the loss function.\n",
    "\n",
    "As you might expect, PyTorch provides an efficient and tensor-friendly implementation of cross-entropy as part of the `torch.nn.functional` package. Moreover, it also performs softmax internally, so we can directly pass in the model's outputs without converting them into probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1208, -0.2691, -0.5300, -0.1142, -0.0899,  0.3923,  0.1140,  0.1554,\n",
       "          0.0325, -0.3034],\n",
       "        [-0.1093, -0.0547, -0.2469, -0.0045,  0.1374,  0.3097,  0.2427,  0.2564,\n",
       "         -0.1285, -0.1923]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.cross_entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3025, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a loss batch whose function will be to get loss of a batch, and optimize model if told to and give accuracy metrics if told to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_fn, xb, yb, opt = None, metrics = None):\n",
    "    preds = model(xb)\n",
    "    loss = loss_fn(preds, yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    metric_result = None\n",
    "    if metrics is not None:\n",
    "        metric_result = metrics(preds, yb)\n",
    "        \n",
    "    return loss.item(), len(xb), metric_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, valid_dl, metric=None):\n",
    "    with torch.no_grad():\n",
    "        # Pass each batch through the model\n",
    "        results = [loss_batch(model, loss_fn, xb, yb, metrics=metric) for xb, yb in valid_dl]\n",
    "        # Separate losses, counts and metrics\n",
    "        losses, nums, metrics = zip(*results)\n",
    "        # Total size of the dataset\n",
    "        total = np.sum(nums)\n",
    "        # Avg. loss across batches\n",
    "        avg_loss = np.sum(np.multiply(losses, nums)) / total\n",
    "        avg_metric = None\n",
    "        if metric is not None:\n",
    "            # Avg. of metric across batches\n",
    "            avg_metric = np.sum(np.multiply(metrics, nums)) / total\n",
    "    return avg_loss, total, avg_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.sum(preds==labels).item()/len(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3231, Accuracy: 0.0922\n"
     ]
    }
   ],
   "source": [
    "val_loss, total, val_acc = evaluate(model, loss_fn, val_loader, metric = accuracy)\n",
    "print ('Loss: {:.4f}, Accuracy: {:.4f}'.format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have checked our validation dataset, we can actually move on to our actual fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_fn, opt, train_dl, valid_dl, metric = None):\n",
    "    for epoch in range(epochs):\n",
    "        # training\n",
    "        for xb, yb in train_dl:\n",
    "            loss,_,_ = loss_batch(model,loss_fn,xb,yb,opt=opt)\n",
    "        \n",
    "        # Evaluation\n",
    "        result = evaluate(model, loss_fn, valid_dl, metric)\n",
    "        val_loss, total, val_metric = result\n",
    "        \n",
    "        # Seeing Progress\n",
    "        if metric is None:\n",
    "            print ('Epoch [{}/{}], Loss: {:.4f}'.format (epoch+1, epochs, val_loss))\n",
    "        else:\n",
    "            print ('Epoch [{}/{}], Loss: {:.4f}, {}: {:.4f}'.format (epoch+1, epochs, val_loss, metric.__name__, val_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make model again\n",
    "model = MnistModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.8864, accuracy: 0.6358\n",
      "Epoch [2/5], Loss: 1.5894, accuracy: 0.7378\n",
      "Epoch [3/5], Loss: 1.3801, accuracy: 0.7708\n",
      "Epoch [4/5], Loss: 1.2293, accuracy: 0.7912\n",
      "Epoch [5/5], Loss: 1.1175, accuracy: 0.8050\n"
     ]
    }
   ],
   "source": [
    "fit(5,model, loss_fn, optimizer, train_loader, val_loader, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In just 5 epochs, our accuracy is now 80%!! \n",
    "\n",
    "The accuracy jumps quite high in very less epochs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.0318, accuracy: 0.8148\n",
      "Epoch [2/5], Loss: 0.9642, accuracy: 0.8217\n",
      "Epoch [3/5], Loss: 0.9096, accuracy: 0.8275\n",
      "Epoch [4/5], Loss: 0.8645, accuracy: 0.8315\n",
      "Epoch [5/5], Loss: 0.8267, accuracy: 0.8338\n"
     ]
    }
   ],
   "source": [
    "fit(5,model, loss_fn, optimizer, train_loader, val_loader, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Relation of accuracy with epochs](./data/download.png) <br>\n",
    "Relation of accuracy with epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite clear from the above picture that the model probably won't cross the accuracy threshold of 90% even after training for a very long time. One possible reason for this is that the learning rate might be too high. The model's parameters may be \"bouncing\" around the optimal set of parameters for the lowest loss. You can try reducing the learning rate and training for a few more epochs to see if it helps.\n",
    "\n",
    "The more likely reason that **the model just isn't powerful enough**. If you remember our initial hypothesis, we have assumed that the output (in this case the class probabilities) is a **linear function** of the input (pixel intensities), obtained by perfoming a matrix multiplication with the weights matrix and adding the bias. This is a fairly weak assumption, as there may not actually exist a linear relationship between the pixel intensities in an image and the digit it represents. While it works reasonably well for a simple dataset like MNIST (getting us to 85% accuracy), we need more sophisticated models that can capture non-linear relationships between image pixels and labels for complex tasks like recognizing everyday objects, animals etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MNIST(root=\"data/\", train=False, transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2204035d880>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[0]\n",
    "plt.imshow(img[0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_img(img, model):\n",
    "    xb = img.unsqueeze(0)\n",
    "    yb = model(xb)\n",
    "    _, preds = torch.max(yb, dim=1)\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 7 Predicted: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[0]\n",
    "plt.imshow(img[0], cmap=\"gray\")\n",
    "print(\"Label:\",label,\"Predicted:\",predict_img(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0 Predicted: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAANrUlEQVR4nO3df4gU9xnH8c+jbf+x/UPrVcyPaluDQQqNxZhCg0lTWjQQvP6RRgnBksKZYKKBQisKqaEUQtKm/0SUCwm9ljalYNIeIq2pSG1ASs6QH+aubX6gVrmcMUIakRCjT//YMZx6853LzszOns/7BcfuzrM7+2SST2Z2vzvzNXcXgMvftKYbANAZhB0IgrADQRB2IAjCDgTxqU6+mZnx1T9QM3e3iZaX2rOb2XIz+7eZvWFmG8usC0C9rN1xdjObLuk/kr4j6aikFyStdvfhxGvYswM1q2PPvlTSG+7+lrt/KOkPklaWWB+AGpUJ+5WS/jvu8dFs2QXMrM/MhsxsqMR7ASip9i/o3L1fUr/EYTzQpDJ79mOSrh73+KpsGYAuVCbsL0i6xsy+ZGafkbRK0mA1bQGoWtuH8e7+kZndJ+mvkqZLesrdX6usMwCVanvora034zM7ULtaflQDYOog7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiCIjk7ZjM6bMWNGsv7oo48m62vXrk3WDxw4kKzffvvtubXDhw8nX4tqsWcHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSCYxfUyt2DBgmR9ZGSk1PqnTUvvL9avX59b27p1a6n3xsTyZnEt9aMaMzsk6X1JZyV95O5LyqwPQH2q+AXdt9z9RAXrAVAjPrMDQZQNu0vabWYHzKxvoieYWZ+ZDZnZUMn3AlBC2cP4G939mJl9QdJzZvYvd983/gnu3i+pX+ILOqBJpfbs7n4suz0u6VlJS6toCkD12g67mc0ws8+dvy/pu5IOVtUYgGqVOYyfI+lZMzu/nt+7+18q6QqfSE9PT25tYGCgg52gm7Uddnd/S9LXKuwFQI0YegOCIOxAEIQdCIKwA0EQdiAILiU9BaROE5Wk3t7e3NrSpc3+zmnZsmW5taLTY19++eVkfd++fck6LsSeHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeC4FLSU8DZs2eT9XPnznWok0sVjZWX6a1oSuc77rgjWS+aTvpylXcpafbsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAE4+xdYNeuXcn6ihUrkvUmx9nffffdZP3UqVO5tXnz5lXdzgWmT59e6/q7FePsQHCEHQiCsANBEHYgCMIOBEHYgSAIOxAE143vgJtuuilZX7hwYbJeNI5e5zj79u3bk/Xdu3cn6++9915u7ZZbbkm+dvPmzcl6kXvvvTe3tm3btlLrnooK9+xm9pSZHTezg+OWzTKz58zs9ex2Zr1tAihrMofxv5a0/KJlGyXtcfdrJO3JHgPoYoVhd/d9kk5etHilpIHs/oCk3mrbAlC1dj+zz3H30ez+25Lm5D3RzPok9bX5PgAqUvoLOnf31Aku7t4vqV/iRBigSe0OvY2Z2VxJym6PV9cSgDq0G/ZBSWuy+2sk/bmadgDUpfB8djN7WtLNkmZLGpP0U0l/kvRHSV+UdFjS99394i/xJlrXZXkYP3/+/GR9//79yfrs2bOT9TLXZi+69vqOHTuS9YceeihZP336dLKeUnQ+e9F26+npSdY/+OCD3NqDDz6YfO3jjz+erJ85cyZZb1Le+eyFn9ndfXVO6dulOgLQUfxcFgiCsANBEHYgCMIOBEHYgSC4lHQFFixYkKyPjIyUWn/R0NvevXtza6tWrUq+9sSJE2311An3339/sv7YY48l66ntVnRa8LXXXpusv/nmm8l6k7iUNBAcYQeCIOxAEIQdCIKwA0EQdiAIwg4EwaWkp4ChoaFk/e67786tdfM4epHBwcFk/c4770zWr7/++irbmfLYswNBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIyzd0DR+ehFbrjhhoo6mVrMJjwt+2NF27XMdt+yZUuyftddd7W97qawZweCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIBhnr8A999yTrBddoxwTu+2225L1xYsXJ+up7V7076RonH0qKtyzm9lTZnbczA6OW7bFzI6Z2UvZ3631tgmgrMkcxv9a0vIJlv/K3a/L/nZV2xaAqhWG3d33STrZgV4A1KjMF3T3mdkr2WH+zLwnmVmfmQ2ZWfpCagBq1W7Yt0n6iqTrJI1K+mXeE929392XuPuSNt8LQAXaCru7j7n7WXc/J+kJSUurbQtA1doKu5nNHffwe5IO5j0XQHcoHGc3s6cl3SxptpkdlfRTSTeb2XWSXNIhSWvra7H7FY0HR9bT05NbW7RoUfK1mzZtqrqdj73zzjvJ+pkzZ2p776YUht3dV0+w+MkaegFQI34uCwRB2IEgCDsQBGEHgiDsQBCc4opabd68Obe2bt26Wt/70KFDubU1a9YkX3vkyJGKu2kee3YgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIJxdpSya1f6WqMLFy7sUCeXGh4ezq09//zzHeykO7BnB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgGGevgJkl69Omlft/6ooVK9p+bX9/f7J+xRVXtL1uqfifrcnpqrnE94XYswNBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIyzV2Dbtm3J+iOPPFJq/Tt37kzWy4xl1z0OXuf6t2/fXtu6L0eFe3Yzu9rM9prZsJm9ZmYbsuWzzOw5M3s9u51Zf7sA2jWZw/iPJP3I3RdJ+oakdWa2SNJGSXvc/RpJe7LHALpUYdjdfdTdX8zuvy9pRNKVklZKGsieNiCpt6YeAVTgE31mN7P5khZL+qekOe4+mpXeljQn5zV9kvpK9AigApP+Nt7MPitph6QH3P1/42vu7pJ8ote5e7+7L3H3JaU6BVDKpMJuZp9WK+i/c/dnssVjZjY3q8+VdLyeFgFUwVo75cQTWudvDkg66e4PjFv+qKR33f1hM9soaZa7/7hgXek3m6LmzZuXrO/fvz9Z7+npSda7+TTSot7GxsZyayMjI8nX9vWlP/2Njo4m66dPn07WL1fuPuE515P5zP5NSXdJetXMXsqWbZL0sKQ/mtkPJR2W9P0K+gRQk8Kwu/vzkvKuzvDtatsBUBd+LgsEQdiBIAg7EARhB4Ig7EAQhePslb7ZZTrOXmTZsmXJem9vb7K+YcOGZL2bx9nXr1+fW9u6dWvV7UD54+zs2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMbZp4Dly5cn66nzvoumLR4cHEzWi6Z8Lpquenh4OLd25MiR5GvRHsbZgeAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIxtmBywzj7EBwhB0IgrADQRB2IAjCDgRB2IEgCDsQRGHYzexqM9trZsNm9pqZbciWbzGzY2b2UvZ3a/3tAmhX4Y9qzGyupLnu/qKZfU7SAUm9as3HfsrdfzHpN+NHNUDt8n5UM5n52UcljWb33zezEUlXVtsegLp9os/sZjZf0mJJ/8wW3Wdmr5jZU2Y2M+c1fWY2ZGZD5VoFUMakfxtvZp+V9HdJP3f3Z8xsjqQTklzSz9Q61L+7YB0cxgM1yzuMn1TYzezTknZK+qu7PzZBfb6kne7+1YL1EHagZm2fCGOty4c+KWlkfNCzL+7O+56kg2WbBFCfyXwbf6Okf0h6VdL5uYE3SVot6Tq1DuMPSVqbfZmXWhd7dqBmpQ7jq0LYgfpxPjsQHGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiCIwgtOVuyEpMPjHs/OlnWjbu2tW/uS6K1dVfY2L6/Q0fPZL3lzsyF3X9JYAwnd2lu39iXRW7s61RuH8UAQhB0Ioumw9zf8/ind2lu39iXRW7s60lujn9kBdE7Te3YAHULYgSAaCbuZLTezf5vZG2a2sYke8pjZITN7NZuGutH56bI59I6b2cFxy2aZ2XNm9np2O+Ecew311hXTeCemGW902zU9/XnHP7Ob2XRJ/5H0HUlHJb0gabW7D3e0kRxmdkjSEndv/AcYZrZM0ilJvzk/tZaZPSLppLs/nP2Pcqa7/6RLetuiTziNd0295U0z/gM1uO2qnP68HU3s2ZdKesPd33L3DyX9QdLKBvroeu6+T9LJixavlDSQ3R9Q6z+WjsvprSu4+6i7v5jdf1/S+WnGG912ib46oomwXynpv+MeH1V3zffuknab2QEz62u6mQnMGTfN1tuS5jTZzAQKp/HupIumGe+abdfO9Odl8QXdpW50969LWiFpXXa42pW89Rmsm8ZOt0n6ilpzAI5K+mWTzWTTjO+Q9IC7/298rcltN0FfHdluTYT9mKSrxz2+KlvWFdz9WHZ7XNKzan3s6CZj52fQzW6PN9zPx9x9zN3Puvs5SU+owW2XTTO+Q9Lv3P2ZbHHj226ivjq13ZoI+wuSrjGzL5nZZyStkjTYQB+XMLMZ2RcnMrMZkr6r7puKelDSmuz+Gkl/brCXC3TLNN5504yr4W3X+PTn7t7xP0m3qvWN/JuSNjfRQ05fX5b0cvb3WtO9SXparcO6M2p9t/FDSZ+XtEfS65L+JmlWF/X2W7Wm9n5FrWDNbai3G9U6RH9F0kvZ361Nb7tEXx3ZbvxcFgiCL+iAIAg7EARhB4Ig7EAQhB0IgrADQRB2IIj/A8nhboC3dEL1AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[3]\n",
    "plt.imshow(img[0], cmap=\"gray\")\n",
    "print(\"Label:\",label,\"Predicted:\",predict_img(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 3 Predicted: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[18]\n",
    "plt.imshow(img[0], cmap=\"gray\")\n",
    "print(\"Label:\",label,\"Predicted:\",predict_img(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 6 Predicted: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[11]\n",
    "plt.imshow(img[0], cmap=\"gray\")\n",
    "print(\"Label:\",label,\"Predicted:\",predict_img(img, model))\n",
    "\n",
    "# Here we can see that our prediction is wrong and this is the drawback of logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7962945294380188 Accuracy: 0.8443\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size =200)\n",
    "test_loss, total, test_acc = evaluate(model, loss_fn, test_loader, accuracy)\n",
    "print(\"Loss:\", test_loss, \"Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mnist-logistic.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[-0.0162, -0.0094,  0.0020,  ..., -0.0242, -0.0127, -0.0310],\n",
       "                      [ 0.0123, -0.0022,  0.0237,  ...,  0.0270,  0.0167, -0.0134],\n",
       "                      [ 0.0106,  0.0027,  0.0285,  ..., -0.0315, -0.0353,  0.0055],\n",
       "                      ...,\n",
       "                      [-0.0032,  0.0082,  0.0003,  ..., -0.0243, -0.0300,  0.0134],\n",
       "                      [ 0.0189,  0.0354,  0.0159,  ...,  0.0086,  0.0264,  0.0284],\n",
       "                      [ 0.0346,  0.0340, -0.0075,  ...,  0.0279,  0.0251,  0.0028]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.0593,  0.0641,  0.0029, -0.0339,  0.0527, -0.0089,  0.0242,  0.0597,\n",
       "                      -0.0633,  0.0317]))])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to load model create an instance of mnist and then load model\n",
    "model2 = MnistModel()\n",
    "model2.load_state_dict(torch.load('mnist-logistic.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[-0.0162, -0.0094,  0.0020,  ..., -0.0242, -0.0127, -0.0310],\n",
       "                      [ 0.0123, -0.0022,  0.0237,  ...,  0.0270,  0.0167, -0.0134],\n",
       "                      [ 0.0106,  0.0027,  0.0285,  ..., -0.0315, -0.0353,  0.0055],\n",
       "                      ...,\n",
       "                      [-0.0032,  0.0082,  0.0003,  ..., -0.0243, -0.0300,  0.0134],\n",
       "                      [ 0.0189,  0.0354,  0.0159,  ...,  0.0086,  0.0264,  0.0284],\n",
       "                      [ 0.0346,  0.0340, -0.0075,  ...,  0.0279,  0.0251,  0.0028]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.0593,  0.0641,  0.0029, -0.0339,  0.0527, -0.0089,  0.0242,  0.0597,\n",
       "                      -0.0633,  0.0317]))])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dcacb0086e9a4f4eabd41c33bf4faac5ea0a3337ed3f5eff0680afa930572c04"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
